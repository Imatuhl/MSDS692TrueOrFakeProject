base_dir = "/kaggle/input/glove6b/glove.6B.100d.txt"

max_seq_len = 100
max_num_words = 20000

print('Indexing Word Vectors')
def get_coefs(word, *arr): 
    return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(base_dir))

print('Found %s word vectors.' % len(embeddings_index))

beds = np.stack(embeddings_index.values())
emean,estd = beds.mean(), beds.std()
esize = beds.shape[1]

windex = tokenizer.word_index
nb_words = min(max_num_words, len(windex))

embedding_matrix = embedding_matrix = np.random.normal(emean, estd, (nb_words, esize))
for word, i in windex.items():
    if i >= max_num_words: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector
    
    #Defining Glove Neural Network
model2 = models.Sequential()
model2.add(layers.Embedding(max_num_words, output_dim=esize,
                    weights=[embedding_matrix], 
                    input_length=max_seq_len, trainable=False))
model2.add(layers.Dense(128, activation ='relu'))
model2.add(layers.Dense(128, activation ='relu'))
model2.add(layers.Dense(128, activation ='relu'))
model2.add(BatchNormalization())
model2.add(layers.Flatten())
model2.add(layers.Dense(128, activation ='relu'))
model2.add(layers.Dense(1, activation='sigmoid'))

model.summary()

model2.compile (optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])

history = model2.fit(X_train,
                    ytrain,
                    epochs = 15,
                    batch_size=500,
                    validation_data = (X_test, ytest),
                    verbose = 1,
                    callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)])

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) +1)

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

results = model2.evaluate(X_test, ytest)
results

##Visualization
pred2 = model2.predict_classes(X_test)
pred[:5]

confmatrix2 = confusion_matrix(ytest, pred2)
confmatrix2

confmatrix2 = pd.DataFrame(confmatrix2, index = ['FakeNews','Factual'], columns=['FakeNews','Factual'])
plt.figure(figsize = (10,10))
sns.heatmap(confmatrix2, cmap='Reds', linecolor = 'black',
           linewidth = 1, annot= True, fmt='',xticklabels=['FakeNews','Factual'],
            yticklabels = ['FakeNews','Factual'])
plt.xlabel("Actual")
plt.ylabel("Predicted")
